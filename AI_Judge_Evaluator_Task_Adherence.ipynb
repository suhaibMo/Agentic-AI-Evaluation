{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Adherence Evaluator\n",
    "\n",
    "# Intent Resolution Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates to how to use task adherence evaluator on agent data. The supported input formats include:\n",
    "- simple data such as strings;\n",
    "- user-agent conversations in the form of list of agent messages. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 10 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Task Adherence Evaluator\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task Adherence evaluator measures how well the agent adheres to their assigned tasks or predefined goal.\n",
    "\n",
    "The scoring is on a 1-5 integer scale and is as follows:\n",
    "\n",
    "  - Score 1: Fully Inadherent\n",
    "  - Score 2: Barely Adherent\n",
    "  - Score 3: Moderately Adherent\n",
    "  - Score 4: Mostly Adherent\n",
    "  - Score 5: Fully Adherent\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "  - Query    : The user query. Either a string with a user request or a list of messages with previous requests from the user and responses from the assistant, potentially including a system message.\n",
    "  - Response : The response to be evaluated. Either a string or a message with the response from the agent to the last user query.\n",
    "\n",
    "There is a third optional parameter:\n",
    "  - ToolDefinitions : The list of tool definitions the agent can call. This may be useful for the evaluator to better assess if the right tool was called to adhere to user intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Task Adherence Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".credentials.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import TaskAdherenceEvaluator, AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "task_adherence_evaluator = TaskAdherenceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 2.0,\n",
      " 'task_adherence_reason': 'The response partially aligns with the query by '\n",
      "                          'mentioning watering and trimming, but it lacks '\n",
      "                          'depth and omits several key practices necessary for '\n",
      "                          'maintaining a healthy rose garden in summer.',\n",
      " 'task_adherence_result': 'fail',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Failure example, there's only a vague adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"Make sure to water your roses regularly and trim them occasionally.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 4.0,\n",
      " 'task_adherence_reason': 'The response is clear, accurate, and aligns well '\n",
      "                          'with the instructions, providing a comprehensive '\n",
      "                          'guide to summer rose garden care with only minor '\n",
      "                          'room for additional detail.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Success example, full adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"For optimal summer care of your rose garden, start by watering deeply early in the morning to ensure the roots are well-hydrated without encouraging fungal growth. Apply a 2-3 inch layer of organic mulch around the base of the plants to conserve moisture and regulate soil temperature. Fertilize with a balanced rose fertilizer every 4 to 6 weeks to support healthy growth. Prune away any dead or diseased wood to promote good air circulation, and inspect regularly for pests such as aphids or spider mites, treating them promptly with an appropriate organic insecticidal soap. Finally, ensure that your roses receive at least 6 hours of direct sunlight daily for robust flowering.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 5.0,\n",
      " 'task_adherence_reason': \"The response effectively fulfills the user's \"\n",
      "                          'request by providing a specific book recommendation '\n",
      "                          'in the historical fiction genre, along with a brief '\n",
      "                          'summary. It also offers to provide more details or '\n",
      "                          'another suggestion, which enhances the user '\n",
      "                          'experience. The response is well-structured and '\n",
      "                          'adheres to the task requirements.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in literature and at provid can provide book recommendations.\"},\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:00Z\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"I love historical fiction. Can you recommend a good book from that genre?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response = [\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:05Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Let me fetch a recommendation for historical fiction.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:10Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "                \"name\": \"get_book\",\n",
    "                \"arguments\": {\"genre\": \"historical fiction\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:15Z\",\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_result\": '{ \"book\": { \"title\": \"The Pillars of the Earth\", \"author\": \"Ken Follett\", \"summary\": \"A captivating tale set in medieval England that weaves historical events with personal drama.\" } }',\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:20Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Based on our records, I recommend 'The Pillars of the Earth' by Ken Follett. This novel is an excellent example of historical fiction with a rich narrative and well-developed characters. Would you like more details or another suggestion?\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"get_book\",\n",
    "        \"description\": \"Retrieve a book recommendation for a specified genre.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"genre\": {\"type\": \"string\", \"description\": \"The genre for which a book recommendation is requested.\"}\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "result = task_adherence_evaluator(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    tool_definitions=tool_definitions,\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch evaluate and visualize results on Azure AI Foundry\n",
    "Batch evaluate to leverage asynchronous evaluation on a dataset. \n",
    "\n",
    "Optionally, you can go to AI Foundry URL for rich Azure AI Foundry data visualization. You can inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve. Make sure to authenticate to Azure using `az login` in your terminal before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-26 14:51:25 +0100][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-26 14:51:25 +0100][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_task_adherence_20250626_145125_187663, log path: C:\\Users\\sumohammed\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_task_adherence_20250626_145125_187663\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:51:25 +0100   39872 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-06-26 14:51:28 +0100   39872 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-06-26 14:51:28 +0100   39872 execution.bulk     INFO     Average execution time for completed lines: 2.74 seconds. Estimated time for incomplete lines: 10.96 seconds.\n",
      "2025-06-26 14:51:28 +0100   39872 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-06-26 14:51:28 +0100   39872 execution.bulk     INFO     Average execution time for completed lines: 1.55 seconds. Estimated time for incomplete lines: 4.65 seconds.\n",
      "2025-06-26 14:51:29 +0100   39872 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-06-26 14:51:29 +0100   39872 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 2.12 seconds.\n",
      "2025-06-26 14:51:29 +0100   39872 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-06-26 14:51:29 +0100   39872 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 0.85 seconds.\n",
      "2025-06-26 14:51:30 +0100   39872 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-06-26 14:51:30 +0100   39872 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_task_adherence_20250626_145125_187663\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-06-26 14:51:25.234681+01:00\"\n",
      "Duration: \"0:00:05.718741\"\n",
      "Output path: \"C:\\Users\\sumohammed\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_task_adherence_20250626_145125_187663\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.718741\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\sumohammed\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_task_adherence_20250626_145125_187663\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "('AI Foundary URL: '\n",
      " 'https://ai.azure.com/build/evaluation/baa1fa2b-1401-4344-8b77-52352109c28a?wsid=/subscriptions/687537c9-1139-4975-85ff-c4822c224772/resourceGroups/rg-sumohammed-6118_ai/providers/Microsoft.MachineLearningServices/workspaces/sumohammed-0192')\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# This sample files contains the evaluation data in JSONL format. Where each line is a run from agent.\n",
    "# This was saved using agent thread and converter.\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluation_name=\"Task Adherence Evaluation\",\n",
    "    evaluators={\n",
    "        \"task_adherence\": task_adherence_evaluator,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agent_eval3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
